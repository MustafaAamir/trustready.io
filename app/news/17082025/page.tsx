export default function article170825() {
  return (
    <>
      <h1 className="">
        New AI Governance Requirements: What Tech Companies Need to Know in 2025
      </h1>
      <h2 id="1-the-eu-ai-act-a-global-compliance-benchmark">
        1. The EU AI Act: A Global Compliance Benchmark
      </h2>
      <p>
        The European Union&#39;s Artificial Intelligence Act (EU AI Act) stands
        as the world&#39;s first comprehensive legal framework for artificial
        intelligence, establishing a global benchmark for AI governance and
        compliance . Officially adopted in March 2024, the Act introduces a
        risk-based regulatory model that imposes stringent obligations on tech
        companies, with significant penalties for non-compliance . The
        regulation&#39;s extraterritorial reach means that any organization,
        regardless of its location, must comply if its AI systems affect EU
        citizens or are deployed within the EU market . This global
        applicability makes the EU AI Act a critical consideration for tech
        companies worldwide, as it sets a precedent for responsible AI
        development and deployment. The Act&#39;s phased implementation
        timeline, with key deadlines in 2025 and 2026, requires companies to
        take immediate and proactive steps to ensure compliance, including
        conducting risk assessments, enhancing data governance, and establishing
        robust oversight mechanisms . The regulation&#39;s focus on
        transparency, accountability, and fundamental rights aims to foster a
        trustworthy AI ecosystem, but it also presents significant structural,
        technical, and governance-related challenges for businesses,
        particularly those dealing with general-purpose AI (GPAI) models .
      </p>
      <h3 id="1-1-overview-and-significance-for-tech-companies">
        1.1. Overview and Significance for Tech Companies
      </h3>
      <p>
        The EU AI Act is a landmark piece of legislation that establishes a
        uniform legal framework for the development, marketing, and use of AI
        systems within the European Union . Its significance for tech companies
        cannot be overstated, as it introduces a new era of regulatory oversight
        for AI technologies. The Act&#39;s risk-based approach categorizes AI
        systems into four distinct risk levels:{" "}
        <strong>minimal, limited, high, and unacceptable</strong> . This
        classification determines the specific compliance obligations for each
        system, with the most stringent requirements applied to high-risk AI
        systems and a complete ban on those deemed to pose an unacceptable risk
        . The regulation&#39;s scope is broad, covering a wide range of AI
        applications, from simple chatbots to complex generative AI models, and
        it applies to all actors in the AI value chain, including providers,
        deployers, importers, and distributors . For tech companies, this means
        that a thorough understanding of the Act&#39;s provisions is essential
        for navigating the complex regulatory landscape and avoiding significant
        financial and reputational damage. The Act&#39;s emphasis on
        transparency, data quality, and human oversight also has profound
        implications for the design and development of AI systems, requiring
        companies to integrate these principles into their core processes .
      </p>
      <p>
        The EU AI Act&#39;s extraterritorial reach is a particularly critical
        aspect for tech companies operating globally. The regulation applies to
        any organization that places an AI system on the EU market or puts it
        into service, regardless of where the company is located . This means
        that a tech company based in the United States or Asia must comply with
        the EU AI Act if its AI products or services are used by EU citizens or
        within the EU market . This broad applicability has led many to consider
        the EU AI Act a{" "}
        <strong>de facto global standard for AI regulation</strong>, similar to
        the impact of the General Data Protection Regulation (GDPR) on data
        privacy. The Act&#39;s provisions are designed to ensure that AI systems
        are safe, transparent, and respectful of fundamental rights, and it
        imposes significant penalties for non-compliance, including fines of up
        to{" "}
        <strong>
          €35 million or 7% of a company&#39;s global annual turnover
        </strong>
        , whichever is higher . As a result, tech companies must not only
        understand the technical requirements of the Act but also establish
        robust governance structures and compliance programs to ensure that
        their AI systems meet the new standards. The Act&#39;s phased
        implementation timeline, with key deadlines in 2025 and 2026, provides a
        roadmap for compliance, but it also creates a sense of urgency for
        companies to begin their preparations now .
      </p>
      <h3 id="1-2-prohibited-ai-practices">1.2. Prohibited AI Practices</h3>
      <p>
        The EU AI Act takes a firm stance against certain AI practices that are
        deemed to pose an unacceptable risk to individuals and society. As of{" "}
        <strong>February 2, 2025</strong>, the use and development of these
        prohibited AI systems are banned across the European Union . These
        prohibitions are comprehensive, applying to both the creation and the
        mere use of such systems, and they are designed to protect fundamental
        rights, prevent discrimination, and ensure public safety . The list of
        prohibited AI practices includes systems that use subliminal techniques
        to manipulate human behavior, exploit vulnerabilities of specific
        groups, or engage in social scoring . The ban also extends to emotion
        recognition systems in the workplace and biometric categorization
        systems that are based on sensitive characteristics such as race,
        religion, or sexual orientation . By prohibiting these high-risk
        applications, the EU AI Act aims to create a clear ethical boundary for
        AI development and deployment, ensuring that technology is used in a way
        that is consistent with European values and fundamental rights.
      </p>
      <p>
        The prohibition of these AI practices has significant implications for
        tech companies, as it requires them to conduct a thorough review of
        their AI systems to ensure compliance. Companies must establish a
        complete inventory of their AI systems and classify them according to
        the risk categories defined in the Act . This process involves
        identifying any systems that may fall under the prohibited category and
        taking immediate action to remove them from the EU market . The
        Act&#39;s broad definition of prohibited practices means that companies
        must be vigilant in their assessment, as even seemingly benign
        applications could be caught by the ban if they have the potential to
        cause harm or infringe on fundamental rights. For example, a system that
        uses biometric data to infer emotions in a workplace setting would be
        prohibited, even if it is intended to improve employee well-being . The
        penalties for violating these prohibitions are severe, with fines of up
        to{" "}
        <strong>
          €35 million or 7% of a company&#39;s global annual turnover
        </strong>
        , underscoring the importance of compliance . As a result, tech
        companies must not only understand the specific prohibitions but also
        implement robust internal governance processes to ensure that their AI
        systems are developed and used in a responsible and ethical manner.
      </p>
      <p>
        The following table provides a summary of the key prohibited AI
        practices under the EU AI Act, based on information from various sources
        .
      </p>
      <table>
        <thead>
          <tr>
            <th className="text-align:left">Prohibited AI Practice</th>
            <th className="text-align:left">Description</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td className="text-align:left">
              <strong>Subliminal Manipulation</strong>
            </td>
            <td className="text-align:left">
              AI systems that use subliminal techniques beyond a person&#39;s
              consciousness to materially distort their behavior in a way that
              is likely to cause physical or psychological harm.
            </td>
          </tr>
          <tr>
            <td className="text-align:left">
              <strong>Exploitation of Vulnerabilities</strong>
            </td>
            <td className="text-align:left">
              AI systems that exploit the vulnerabilities of a specific group of
              persons, such as children or persons with disabilities, to
              materially distort their behavior in a way that is likely to cause
              physical or psychological harm.
            </td>
          </tr>
          <tr>
            <td className="text-align:left">
              <strong>Social Scoring</strong>
            </td>
            <td className="text-align:left">
              AI systems that evaluate or classify the trustworthiness of
              natural persons over a certain period of time based on their
              social behavior or personal characteristics, leading to
              detrimental or unfavorable treatment in social contexts.
            </td>
          </tr>
          <tr>
            <td className="text-align:left">
              <strong>Emotion Recognition in the Workplace</strong>
            </td>
            <td className="text-align:left">
              AI systems that are used to infer the emotions of a natural person
              in the context of the workplace or educational institutions, with
              the exception of systems used for medical or safety reasons.
            </td>
          </tr>
          <tr>
            <td className="text-align:left">
              <strong>Biometric Categorization</strong>
            </td>
            <td className="text-align:left">
              AI systems that categorize individuals based on their biometric
              data to deduce or infer their race, political opinions, trade
              union membership, religious or philosophical beliefs, sex life, or
              sexual orientation.
            </td>
          </tr>
          <tr>
            <td className="text-align:left">
              <strong>
                Real-time Biometric Identification in Public Spaces
              </strong>
            </td>
            <td className="text-align:left">
              The use of AI systems for real-time biometric identification in
              publicly accessible spaces for law enforcement purposes, with
              limited exceptions for specific and serious crimes.
            </td>
          </tr>
        </tbody>
      </table>
      <h3 id="1-3-high-risk-ai-systems-key-obligations">
        1.3. High-Risk AI Systems: Key Obligations
      </h3>
      <p>
        The EU AI Act imposes a comprehensive set of obligations on high-risk AI
        systems, which are defined as AI systems that have the potential to
        cause significant harm to individuals or society . These systems are
        subject to the most stringent regulatory requirements, and they must
        undergo a conformity assessment before they can be placed on the EU
        market . The Act identifies a wide range of high-risk AI systems,
        including those used in critical infrastructure, education, employment,
        and law enforcement . For tech companies that develop or deploy these
        systems, compliance with the EU AI Act is a complex and multifaceted
        undertaking that requires a deep understanding of the regulation&#39;s
        technical and organizational requirements. The key obligations for
        high-risk AI systems can be grouped into several categories, including
        risk management, data governance, technical documentation, transparency,
        human oversight, and cybersecurity . Each of these areas requires a
        significant investment of time and resources, and companies must be
        prepared to demonstrate their compliance to regulators upon request.
      </p>
      <p>
        The risk-based approach of the EU AI Act means that the specific
        obligations for high-risk AI systems are tailored to the level of risk
        they pose . This approach is designed to balance the need for innovation
        with the need to protect fundamental rights and public safety. However,
        it also creates a complex compliance landscape that can be challenging
        for companies to navigate. For example, a company that develops an AI
        system for credit scoring must not only ensure that the system is
        accurate and fair but also that it is transparent, explainable, and
        subject to human oversight . This requires a holistic approach to AI
        governance that integrates legal, technical, and ethical considerations
        into the entire AI lifecycle, from design and development to deployment
        and monitoring. The penalties for non-compliance are significant, with
        fines of up to{" "}
        <strong>
          €15 million or 3% of a company&#39;s global annual turnover
        </strong>{" "}
        for most violations . As a result, tech companies must take a proactive
        and strategic approach to compliance, investing in the necessary
        resources and expertise to ensure that their high-risk AI systems meet
        the new standards.
      </p>
      <h4 id="1-3-1-risk-management-systems">1.3.1. Risk Management Systems</h4>
      <p>
        A cornerstone of the EU AI Act&#39;s requirements for high-risk AI
        systems is the establishment of a comprehensive risk management system .
        This system must be designed to identify, analyze, and mitigate the
        risks associated with the AI system throughout its entire lifecycle,
        from development to deployment and beyond . The risk management system
        must be a continuous and iterative process, with regular reviews and
        updates to ensure that it remains effective in the face of evolving
        risks and new information. The system must also be documented in detail,
        with clear records of the risk assessment process, the identified risks,
        and the mitigation measures that have been implemented . This
        documentation is essential for demonstrating compliance to regulators
        and for ensuring that the AI system is used in a safe and responsible
        manner. The risk management system must also be integrated into the
        company&#39;s overall governance structure, with clear lines of
        responsibility and accountability for managing AI-related risks.
      </p>
      <p>
        The EU AI Act requires that the risk management system for high-risk AI
        systems be based on a thorough risk assessment that considers the
        potential for harm to individuals and society . This assessment must
        take into account a wide range of factors, including the intended use of
        the AI system, the potential for misuse, and the vulnerability of the
        individuals or groups who may be affected by the system . The risk
        assessment must also consider the potential for bias and discrimination,
        and it must include measures to mitigate these risks . The risk
        management system must also include a plan for post-market monitoring,
        which involves continuously monitoring the performance of the AI system
        in the real world and taking corrective action if any issues arise .
        This ongoing monitoring is essential for ensuring that the AI system
        remains safe and effective over time, and it is a key component of the
        EU AI Act&#39;s risk-based approach to regulation. For tech companies,
        the development and implementation of a robust risk management system is
        a critical step in achieving compliance with the EU AI Act and for
        building trust with customers and regulators.
      </p>
      <h4 id="1-3-2-data-governance-and-management">
        1.3.2. Data Governance and Management
      </h4>
      <p>
        The EU AI Act places a strong emphasis on data governance and management
        for high-risk AI systems, recognizing that the quality of the data used
        to train and operate these systems is a critical factor in their safety
        and effectiveness . The Act requires that high-risk AI systems be
        developed using{" "}
        <strong>
          high-quality datasets that are relevant, representative, and free from
          errors and biases
        </strong>{" "}
        . This means that tech companies must implement robust data governance
        procedures to ensure that their data meets these standards. These
        procedures must cover the entire data lifecycle, from collection and
        preparation to validation and testing . The data governance framework
        must also include measures to mitigate the risk of bias and
        discrimination, which can be introduced into the AI system through the
        training data . This is a particularly important consideration for AI
        systems that are used in sensitive areas such as employment, credit
        scoring, and law enforcement, where biased outcomes can have a
        significant impact on individuals&#39; lives.
      </p>
      <p>
        The EU AI Act also requires that the data used to train and operate
        high-risk AI systems be transparent and auditable . This means that
        companies must be able to provide detailed documentation on the sources
        of their data, the methods used to collect and process it, and the steps
        taken to ensure its quality and integrity . This documentation is
        essential for demonstrating compliance with the Act and for enabling
        regulators to assess the safety and fairness of the AI system. The Act
        also requires that companies implement measures to protect the privacy
        and security of the data they use, in line with the requirements of the
        General Data Protection Regulation (GDPR) . For tech companies, the data
        governance and management requirements of the EU AI Act represent a
        significant challenge, as they require a fundamental shift in the way
        that data is collected, managed, and used in the development of AI
        systems. However, they also present an opportunity for companies to
        build more trustworthy and reliable AI systems that are better aligned
        with the values of their customers and society as a whole.
      </p>
      <h4 id="1-3-3-technical-documentation-and-record-keeping">
        1.3.3. Technical Documentation and Record-Keeping
      </h4>
      <p>
        The EU AI Act mandates that providers of high-risk AI systems create and
        maintain comprehensive technical documentation to demonstrate compliance
        with the regulation&#39;s requirements . This documentation must be
        detailed and up-to-date, and it must be made available to regulators
        upon request . The technical documentation must include a wide range of
        information, such as a general description of the AI system, its
        intended purpose, and the risk management system that has been
        implemented . It must also include detailed information on the data used
        to train and test the system, the algorithms and models used, and the
        measures taken to ensure the system&#39;s accuracy, robustness, and
        cybersecurity . The documentation must also include clear and
        comprehensive user instructions that explain how to use the AI system
        safely and effectively, as well as information on its limitations and
        potential risks .
      </p>
      <p>
        In addition to the technical documentation, the EU AI Act also requires
        that providers of high-risk AI systems maintain detailed records of the
        system&#39;s operation . This includes logging the system&#39;s
        decisions and actions, as well as any incidents or malfunctions that may
        occur . This record-keeping is essential for ensuring the traceability
        and auditability of the AI system, and it is a key component of the
        Act&#39;s risk-based approach to regulation. The records must be stored
        in a secure and accessible manner, and they must be made available to
        regulators upon request . For tech companies, the documentation and
        record-keeping requirements of the EU AI Act represent a significant
        administrative burden, as they require a high level of detail and a
        systematic approach to information management. However, they also
        provide a valuable opportunity for companies to improve their internal
        processes and to build more transparent and accountable AI systems.
      </p>
      <h4 id="1-3-4-transparency-and-user-information">
        1.3.4. Transparency and User Information
      </h4>
      <p>
        Transparency is a core principle of the EU AI Act, and it is a key
        requirement for all high-risk AI systems . The Act mandates that
        providers of these systems must ensure that they are transparent and
        that users are provided with clear and comprehensive information about
        how they work . This includes providing information on the AI
        system&#39;s capabilities and limitations, as well as its intended
        purpose and potential risks . The information must be presented in a
        clear and understandable way, and it must be easily accessible to users
        . The Act also requires that users are informed when they are
        interacting with an AI system, and that they are able to understand how
        the system is making its decisions . This is particularly important for
        AI systems that are used in sensitive areas such as employment and
        credit scoring, where the decisions made by the AI system can have a
        significant impact on individuals&#39; lives.
      </p>
      <p>
        The EU AI Act also requires that providers of high-risk AI systems
        provide clear and comprehensive user instructions that explain how to
        use the system safely and effectively . These instructions must include
        information on the system&#39;s limitations and potential risks, as well
        as guidance on how to interpret the system&#39;s outputs . The
        instructions must also include information on the human oversight
        measures that have been implemented, and how users can exercise their
        right to human intervention . For tech companies, the transparency and
        user information requirements of the EU AI Act represent a significant
        challenge, as they require a fundamental shift in the way that AI
        systems are designed and communicated to users. However, they also
        present an opportunity for companies to build more trustworthy and
        user-friendly AI systems that are better aligned with the needs and
        expectations of their customers.
      </p>
      <h4 id="1-3-5-human-oversight">1.3.5. Human Oversight</h4>
      <p>
        The EU AI Act places a strong emphasis on human oversight for high-risk
        AI systems, recognizing that human intervention is essential for
        ensuring the safety and fairness of these systems . The Act requires
        that high-risk AI systems be designed and developed in such a way that
        they can be effectively overseen by natural persons during the period in
        which the AI system is in use . This means that tech companies must
        implement measures to ensure that humans can understand the AI
        system&#39;s decisions and intervene if necessary to prevent harm . The
        level of human oversight required will depend on the specific risks
        associated with the AI system, but it must be sufficient to ensure that
        the system is used in a safe and responsible manner . The human
        oversight measures must be documented in detail, and they must be made
        available to regulators upon request .
      </p>
      <p>
        The EU AI Act also requires that users of high-risk AI systems are able
        to exercise their right to human intervention . This means that
        individuals who are affected by the decisions of an AI system must be
        able to request a review of the decision by a human, and to challenge
        the decision if they believe it is unfair or incorrect . This is a
        particularly important right for individuals who are subject to
        decisions made by AI systems in sensitive areas such as employment,
        credit scoring, and law enforcement . For tech companies, the human
        oversight requirements of the EU AI Act represent a significant
        challenge, as they require a fundamental shift in the way that AI
        systems are designed and operated. However, they also present an
        opportunity for companies to build more trustworthy and accountable AI
        systems that are better aligned with the values of their customers and
        society as a whole.
      </p>
      <h4 id="1-3-6-accuracy-robustness-and-cybersecurity">
        1.3.6. Accuracy, Robustness, and Cybersecurity
      </h4>
      <p>
        The EU AI Act requires that high-risk AI systems are designed and
        developed to achieve an appropriate level of accuracy, robustness, and
        cybersecurity . This means that tech companies must implement measures
        to ensure that their AI systems are reliable, resilient, and secure
        against both accidental and malicious attacks . The level of accuracy,
        robustness, and cybersecurity required will depend on the specific risks
        associated with the AI system, but it must be sufficient to ensure that
        the system is safe and effective in its intended use . The measures
        taken to ensure these qualities must be documented in detail, and they
        must be made available to regulators upon request .
      </p>
      <p>
        The EU AI Act also requires that providers of high-risk AI systems
        implement a post-market monitoring system to continuously monitor the
        performance of the AI system in the real world . This monitoring system
        must be designed to detect any issues with the system&#39;s accuracy,
        robustness, or cybersecurity, and to take corrective action if any
        issues arise . The monitoring system must also be designed to collect
        and analyze data on the system&#39;s performance, and to use this data
        to improve the system over time . For tech companies, the accuracy,
        robustness, and cybersecurity requirements of the EU AI Act represent a
        significant challenge, as they require a high level of technical
        expertise and a systematic approach to risk management. However, they
        also present an opportunity for companies to build more reliable and
        trustworthy AI systems that are better aligned with the needs and
        expectations of their customers.
      </p>
      <h3 id="1-4-general-purpose-ai-gpai-models">
        1.4. General-Purpose AI (GPAI) Models
      </h3>
      <p>
        The EU AI Act introduces a specific regulatory regime for
        general-purpose AI (GPAI) models, which are defined as AI models that
        can perform a wide range of tasks and are becoming the basis for many AI
        systems in the EU . These models, which include large language models
        and multimodal models, are subject to a set of obligations that are
        designed to ensure their safe and trustworthy development and deployment
        . The obligations for GPAI models are tiered, with more stringent
        requirements for models that are classified as posing a systemic risk .
        The classification of a GPAI model as having systemic risk is based on
        technical criteria such as the amount of computing power used to train
        the model, with a threshold of{" "}
        <strong>10^25 floating point operations (FLOPs)</strong> being a key
        indicator . The EU AI Act&#39;s provisions for GPAI models are designed
        to address the unique challenges posed by these powerful and versatile
        technologies, and they have significant implications for the tech
        companies that develop and use them.
      </p>
      <p>
        The regulatory framework for GPAI models is a key component of the EU AI
        Act&#39;s risk-based approach to AI regulation. It recognizes that while
        GPAI models have the potential to drive innovation and economic growth,
        they also pose significant risks to fundamental rights, public safety,
        and societal well-being . The Act&#39;s provisions for GPAI models are
        designed to mitigate these risks by promoting transparency,
        accountability, and responsible innovation . For tech companies, the
        GPAI provisions of the EU AI Act represent a significant compliance
        challenge, as they require a deep understanding of the regulation&#39;s
        technical and legal requirements. However, they also present an
        opportunity for companies to build more trustworthy and reliable GPAI
        models that are better aligned with the values of their customers and
        society as a whole.
      </p>
      <h4 id="1-4-1-obligations-for-gpai-model-providers">
        1.4.1. Obligations for GPAI Model Providers
      </h4>
      <p>
        Providers of GPAI models are subject to a specific set of obligations
        under the EU AI Act, which are designed to ensure the transparency and
        accountability of these powerful technologies . These obligations, which
        took effect in <strong>August 2025</strong>, require providers to
        maintain comprehensive technical documentation that makes the
        model&#39;s development, training, and evaluation traceable . This
        documentation must include a detailed summary of the training data used,
        including the sources of the data and the methods used to process it .
        Providers are also required to publish a summary of the content used for
        training, which must include information on the data types, sources, and
        preprocessing methods . This requirement is designed to promote
        transparency and to enable downstream users to understand the
        capabilities and limitations of the GPAI model.
      </p>
      <p>
        In addition to the documentation requirements, providers of GPAI models
        are also required to prepare transparency reports that describe the
        model&#39;s capabilities, limitations, and potential risks . These
        reports must also provide guidance for integrators on how to use the
        model safely and effectively . Providers are also required to establish
        a policy to respect EU copyright law, particularly with regard to the
        use of copyright-protected content in the training data . This policy
        must be designed to ensure that the use of such content is legally
        permissible and that the rights of copyright holders are protected . For
        providers of GPAI models with systemic risk, the obligations are even
        more stringent. These providers must report their models to the European
        Commission, undergo structured evaluation and testing procedures, and
        permanently document any security incidents . They are also subject to
        increased requirements in the area of cybersecurity and monitoring .
      </p>
      <h4 id="1-4-2-obligations-for-downstream-modifiers-and-users">
        1.4.2. Obligations for Downstream Modifiers and Users
      </h4>
      <p>
        The EU AI Act also imposes obligations on downstream providers and
        modifiers of GPAI models, recognizing that these actors play a critical
        role in the AI value chain . A company that substantially modifies an
        existing GPAI model, for example by retraining or fine-tuning it, will
        itself become a provider for regulatory purposes . This means that the
        company will be subject to all of the obligations that apply to the
        original provider of the GPAI model . A modification is considered
        substantial if it significantly changes the functionality, performance,
        or risks of the model, and it does not merely amount to integration or
        use . This provision is designed to ensure that the risks associated
        with GPAI models are managed throughout the AI lifecycle, and it has
        significant implications for companies that use these models to develop
        their own AI applications.
      </p>
      <p>
        Users of AI systems, particularly those that are classified as
        high-risk, are also subject to a set of obligations under the EU AI Act
        . These users, who are referred to as &quot;deployers&quot; in the Act,
        are required to maintain a complete inventory of the AI systems they use
        . They must also ensure that they are not using any prohibited AI
        systems . For high-risk AI systems, deployers are subject to additional
        obligations, such as conducting data protection impact assessments and
        implementing internal monitoring procedures . The more extensive
        transparency obligations for AI system users, such as the requirement to
        label AI-generated content, will not become binding until{" "}
        <strong>August 2026</strong> . For tech companies that use AI systems in
        their operations, these obligations represent a significant compliance
        challenge, as they require a deep understanding of the regulation&#39;s
        requirements and a systematic approach to risk management.
      </p>
      <h3 id="1-5-implementation-timeline-and-key-deadlines">
        1.5. Implementation Timeline and Key Deadlines
      </h3>
      <p>
        The EU AI Act has a staggered implementation timeline, with different
        provisions coming into effect at different times over a three-year
        period . This phased approach is designed to give companies time to
        prepare for the new requirements, but it also creates a complex
        compliance landscape that can be challenging to navigate . The key
        deadlines for compliance are as follows:
      </p>
      <ul>
        <li>
          <strong>February 2, 2025:</strong> The prohibitions on certain AI
          systems and the requirements for AI literacy come into effect .
        </li>
        <li>
          <strong>August 2, 2025:</strong> The obligations for general-purpose
          AI (GPAI) models take effect .
        </li>
        <li>
          <strong>August 2, 2026:</strong> The remaining provisions of the Act
          take effect, including the full set of obligations for high-risk AI
          systems .
        </li>
        <li>
          <strong>August 2, 2027:</strong> The obligations for GPAI models that
          were already on the market before August 2025 become applicable .
        </li>
      </ul>
      <p>
        This timeline provides a roadmap for compliance, but it is important to
        note that the specific deadlines that apply to a company will depend on
        its role in the AI value chain and the type of AI systems it develops or
        uses . For example, a company that develops a new GPAI model will need
        to comply with the new requirements by August 2025, while a company that
        uses a high-risk AI system will have until August 2026 to achieve full
        compliance . The European Commission is also expected to issue further
        guidance and technical standards to help companies comply with the new
        requirements, and it is important for companies to stay up-to-date on
        these developments .
      </p>
      <h4 id="1-5-1-prohibited-ai-systems-ban-february-2025-">
        1.5.1. Prohibited AI Systems Ban (February 2025)
      </h4>
      <p>
        The first major deadline under the EU AI Act was{" "}
        <strong>February 2, 2025</strong>, when the prohibitions on certain AI
        systems came into effect . This deadline marked a significant milestone
        in the regulation of AI in the EU, as it introduced a clear and
        enforceable ban on a range of high-risk AI practices . The prohibited AI
        systems include those that use subliminal techniques to manipulate human
        behavior, exploit the vulnerabilities of specific groups, or engage in
        social scoring . The ban also extends to emotion recognition systems in
        the workplace and biometric categorization systems that are based on
        sensitive characteristics . The prohibitions are comprehensive, applying
        to both the development and the use of these systems, and they are
        designed to protect fundamental rights and prevent discrimination .
      </p>
      <p>
        For tech companies, the February 2025 deadline was a critical moment, as
        it required them to take immediate action to ensure compliance with the
        new prohibitions. This included conducting a thorough review of their AI
        systems to identify any that may fall under the prohibited category, and
        taking steps to remove them from the EU market . The penalties for
        violating these prohibitions are severe, with fines of up to{" "}
        <strong>
          €35 million or 7% of a company&#39;s global annual turnover
        </strong>
        , whichever is higher . As a result, the February 2025 deadline was a
        major compliance challenge for many tech companies, and it highlighted
        the need for a proactive and strategic approach to AI governance.
      </p>
      <h4 id="1-5-2-codes-of-practice-for-gpai-may-2025-">
        1.5.2. Codes of Practice for GPAI (May 2025)
      </h4>
      <p>
        In May 2025, the European Commission is expected to publish the final
        version of the <strong>General-Purpose AI Code of Practice</strong> .
        This Code of Practice is a voluntary compliance tool that is designed to
        help providers of GPAI models meet their obligations under the EU AI Act
        . The Code is being developed through a multi-stakeholder process, with
        input from industry, academia, and civil society, and it is intended to
        provide practical guidance on how to comply with the Act&#39;s
        requirements for transparency, copyright, and safety . The Code of
        Practice is not legally binding, but it is expected to become the
        &quot;best practice&quot; benchmark for compliance, and it will be used
        by regulators to assess whether a company has taken appropriate measures
        to meet its obligations .
      </p>
      <p>
        The Code of Practice is a key component of the EU AI Act&#39;s
        regulatory framework for GPAI models, and it is designed to provide a
        flexible and adaptable approach to compliance. The Code is divided into
        three chapters, covering transparency, copyright, and safety and
        security . The transparency chapter requires providers to maintain
        up-to-date documentation for their GPAI models, while the copyright
        chapter requires them to develop a robust copyright policy . The safety
        and security chapter, which only applies to GPAI models with systemic
        risk, requires providers to conduct model evaluations and to implement
        measures to mitigate systemic risks . For tech companies that develop or
        use GPAI models, the Code of Practice will be an essential resource for
        navigating the complex regulatory landscape and for ensuring compliance
        with the EU AI Act.
      </p>
      <h4 id="1-5-3-full-implementation-and-compliance-august-2025-">
        1.5.3. Full Implementation and Compliance (August 2025)
      </h4>
      <p>
        <strong>August 2, 2025</strong>, is a key deadline under the EU AI Act,
        as it marks the date when a number of important provisions come into
        effect . This includes the obligations for providers of GPAI models, who
        will be required to comply with the new requirements for transparency,
        documentation, and copyright . The August 2025 deadline also marks the
        date when the EU AI Office and the national competent authorities will
        be fully operational, and when they will begin to enforce the new
        regulations . For tech companies, the August 2025 deadline is a critical
        moment, as it requires them to have a comprehensive compliance program
        in place to ensure that they are meeting their obligations under the
        Act.
      </p>
      <p>
        The August 2025 deadline is particularly important for providers of GPAI
        models, as they will be subject to a new set of regulatory requirements
        that are designed to ensure the transparency and accountability of these
        powerful technologies . These requirements include the need to maintain
        comprehensive technical documentation, to publish a summary of the
        content used for training, and to establish a policy to respect EU
        copyright law . For providers of GPAI models with systemic risk, the
        obligations are even more stringent, and they include the need to report
        their models to the European Commission and to undergo structured
        evaluation and testing procedures . The August 2025 deadline is a major
        compliance challenge for many tech companies, and it highlights the need
        for a proactive and strategic approach to AI governance.
      </p>
      <h3 id="1-6-enforcement-and-penalties">1.6. Enforcement and Penalties</h3>
      <p>
        The EU AI Act establishes a robust enforcement framework to ensure that
        companies comply with the new regulations. The enforcement of the Act
        will be carried out by a combination of the European Commission, the EU
        AI Office, and the national competent authorities in each member state .
        These authorities will have the power to conduct investigations, to
        request information from companies, and to impose penalties for
        non-compliance . The penalties for violating the EU AI Act are
        significant, and they are designed to be a strong deterrent against
        non-compliance. The fines for non-compliance can be as high as{" "}
        <strong>
          €35 million or 7% of a company&#39;s global annual turnover
        </strong>
        , whichever is higher, for violations of the prohibited AI practices .
        For most other violations, including non-compliance with the
        requirements for high-risk AI systems, the fines can be as high as{" "}
        <strong>
          €15 million or 3% of a company&#39;s global annual turnover
        </strong>{" "}
        .
      </p>
      <p>
        The enforcement of the EU AI Act will be a key priority for regulators,
        and they have made it clear that they will be closely monitoring the
        implementation of the new regulations . The European Commission has also
        established a central point of contact, the &quot;AI Service Desk,&quot;
        to provide guidance and support to companies, particularly small and
        medium-sized enterprises, on how to comply with the Act . For tech
        companies, the enforcement and penalty provisions of the EU AI Act
        represent a significant risk, and they highlight the importance of
        taking a proactive and strategic approach to compliance. Companies that
        fail to comply with the new regulations face not only the risk of
        significant financial penalties but also the risk of reputational damage
        and loss of market access. As a result, it is essential for tech
        companies to invest in the necessary resources and expertise to ensure
        that their AI systems meet the new standards.
      </p>
      <h2 id="2-ai-governance-in-the-united-states-a-patchwork-of-regulations">
        2. AI Governance in the United States: A Patchwork of Regulations
      </h2>
      <p>
        In contrast to the comprehensive, risk-based approach of the EU AI Act,
        the United States has adopted a more fragmented and sectoral approach to
        AI governance. There is currently no single, overarching federal law
        that regulates AI in the U.S. Instead, the regulatory landscape is a
        patchwork of federal initiatives, state-level legislation, and
        industry-specific rules . This approach is characterized by a focus on
        promoting innovation and technological leadership, while also addressing
        specific risks and harms that may arise from the use of AI. The U.S.
        government has issued a number of executive orders and policy documents
        that set out its vision for AI governance, but these are generally not
        legally binding. The most significant of these is the &quot;Blueprint
        for an AI Bill of Rights,&quot; which outlines a set of principles for
        the design, use, and deployment of AI systems . The U.S. has also
        established the National Institute of Standards and Technology (NIST) AI
        Risk Management Framework, which is a voluntary framework that provides
        guidance on how to manage the risks associated with AI .
      </p>
      <p>
        At the state level, there is a growing number of AI-related laws and
        regulations, particularly in the area of data privacy. California has
        been a leader in this area, with the California Consumer Privacy Act
        (CCPA) and the California Privacy Rights Act (CPRA) imposing strict
        requirements on companies that collect and use personal information .
        These laws have significant implications for AI systems, as they require
        companies to be transparent about their data collection and use
        practices and to provide consumers with certain rights over their
        personal information. In addition to data privacy laws, a number of
        states are also considering or have passed laws that specifically target
        AI systems. For example, Colorado has passed a law that regulates the
        use of AI in &quot;consequential decisions,&quot; such as those related
        to employment, housing, and credit . This law requires companies to
        conduct impact assessments of their AI systems and to provide consumers
        with certain rights, such as the right to opt out of the use of AI in
        certain decisions.
      </p>
      <h3 id="2-1-federal-initiatives-and-the-nist-ai-risk-management-framework">
        2.1. Federal Initiatives and the NIST AI Risk Management Framework
      </h3>
      <p>
        At the federal level, the U.S. government has taken a number of steps to
        promote the responsible development and use of AI. However, these
        initiatives have generally been voluntary and have not resulted in a
        comprehensive, legally binding regulatory framework. The most
        significant of these initiatives is the{" "}
        <strong>NIST AI Risk Management Framework (AI RMF 1.0)</strong> , which
        was developed by the National Institute of Standards and Technology
        (NIST) . The AI RMF is a voluntary framework that provides guidance on
        how to manage the risks associated with AI systems. It is designed to be
        a flexible and adaptable tool that can be used by organizations of all
        sizes and in all sectors. The framework is based on a number of key
        principles, including the need to be transparent, accountable, and fair
        in the design and use of AI systems. It also emphasizes the importance
        of human oversight and the need to ensure that AI systems are robust and
        secure.
      </p>
      <p>
        The AI RMF is organized into four key functions:{" "}
        <strong>Govern, Map, Measure, and Manage</strong>. The Govern function
        is about establishing a culture of risk management within an
        organization and ensuring that there is clear accountability for AI
        risks. The Map function is about identifying and understanding the risks
        associated with a particular AI system. The Measure function is about
        developing and implementing metrics to assess the performance and risks
        of an AI system. The Manage function is about taking action to mitigate
        the identified risks and to ensure that the AI system is used in a safe
        and responsible manner. The AI RMF is not a legally binding document,
        but it is widely seen as a best practice for AI governance in the U.S.
        It is also likely to be used as a reference point for future AI
        regulations, both at the federal and state level.
      </p>
      <h3 id="2-2-california-s-ai-and-data-privacy-laws-effective-january-1-2025-">
        2.2. California&#39;s AI and Data Privacy Laws (Effective January 1,
        2025)
      </h3>
      <p>
        California has emerged as a leader in AI and data privacy regulation in
        the United States, with a number of new laws coming into effect in 2025
        that will have a significant impact on tech companies. These laws are
        designed to protect consumers from the potential harms of AI and to
        ensure that their personal information is used in a responsible and
        transparent manner. The most significant of these laws is the{" "}
        <strong>California Privacy Rights Act (CPRA)</strong> , which builds on
        the California Consumer Privacy Act (CCPA) and introduces a number of
        new requirements for companies that collect and use personal information
        . The CPRA gives consumers new rights over their personal information,
        including the right to correct inaccurate information and the right to
        limit the use of their sensitive personal information. The CPRA also
        establishes a new enforcement agency, the California Privacy Protection
        Agency (CPPA), which is responsible for enforcing the law and for
        issuing new regulations.
      </p>
      <p>
        In addition to the CPRA, California has also passed a number of laws
        that specifically target AI systems. For example,{" "}
        <strong>Assembly Bill 2905 (AB 2905)</strong> requires companies to
        disclose when they are using AI-generated voices in automated calls to
        customers . The disclosure must be made at the beginning of the call and
        must be clear enough that a reasonable person would understand that they
        are talking to an AI. Another law, which is set to take effect on{" "}
        <strong>October 1, 2025</strong>, will regulate the use of AI in hiring
        and employment decisions . This law will require companies to test their
        AI systems for bias, to provide transparency about how decisions are
        made, and to give employees ways to challenge AI-driven decisions. These
        laws are a clear indication that California is taking a proactive
        approach to AI regulation and that it is committed to protecting
        consumers and workers from the potential harms of AI.
      </p>
      <h4 id="2-2-1-assembly-bill-1008-ab-1008-">
        2.2.1. Assembly Bill 1008 (AB 1008)
      </h4>
      <p>
        Assembly Bill 1008 (AB 1008) is a California law that is designed to
        regulate the use of automated decision systems (ADS) in
        &quot;consequential decisions.&quot; The bill is still in the
        legislative process, but it has the potential to have a significant
        impact on tech companies that develop or deploy AI systems in
        California. The bill is modeled on the Colorado AI Act, but it goes
        further in a number of respects. For example, it places significant
        responsibilities on both AI developers and deployers, including the
        requirement to conduct performance evaluations, to undergo third-party
        audits, and to provide transparency to affected individuals . The bill
        also gives individuals the right to opt out of the use of ADS in certain
        decisions and imposes data management rules on companies that use these
        systems. The penalties for non-compliance with the bill are significant,
        with fines of up to <strong>$25,000 per violation</strong> .
      </p>
      <p>
        The bill defines a &quot;consequential decision&quot; as a decision that
        has a legal or similarly significant effect on an individual&#39;s life,
        such as a decision related to employment, education, housing, or credit.
        The bill&#39;s definition of an ADS is also broad, and it includes any
        system that uses AI to make or assist in making a consequential
        decision. The bill&#39;s requirements are designed to ensure that ADS
        are used in a way that is fair, transparent, and accountable. The
        bill&#39;s supporters argue that it is necessary to protect consumers
        from the potential harms of AI, such as discrimination and bias.
        However, the bill&#39;s opponents argue that it could stifle innovation
        and make it more difficult for companies to deliver innovative,
        AI-powered services .
      </p>
      <h4 id="2-2-2-senate-bill-1120-sb-1120-">
        2.2.2. Senate Bill 1120 (SB 1120)
      </h4>
      <p>
        Senate Bill 1120 (SB 1120) is another California law that is designed to
        regulate the use of automated decision systems (ADS). The bill is still
        in the legislative process, but it is similar to AB 1008 in a number of
        respects. The bill is designed to protect individuals from
        discrimination that may result from the use of ADS. The bill requires
        developers and deployers of high-risk ADS to conduct impact assessments
        before releasing or implementing these systems. The impact assessments
        must evaluate the potential risks and effects of the ADS on consumers.
        The bill also requires companies to notify individuals who are affected
        by high-risk automated decisions and to establish a risk governance
        program to oversee the responsible use of these technologies .
      </p>
      <p>
        The bill&#39;s definition of a high-risk ADS is similar to the
        definition of a consequential decision in AB 1008, and it includes
        systems that are used to make decisions in areas such as employment,
        housing, and credit. The bill&#39;s requirements are designed to ensure
        that ADS are used in a way that is fair, transparent, and accountable.
        The bill&#39;s supporters argue that it is necessary to protect
        consumers from the potential harms of AI, such as discrimination and
        bias. However, the bill&#39;s opponents argue that it could stifle
        innovation and make it more difficult for companies to deliver
        innovative, AI-powered services . The bill is still in the early stages
        of the legislative process, and it is not yet clear what its final form
        will be. However, it is a clear indication that California is taking a
        proactive approach to AI regulation and that it is committed to
        protecting consumers from the potential harms of AI.
      </p>
      <h3 id="2-3-the-algorithmic-accountability-act">
        2.3. The Algorithmic Accountability Act
      </h3>
      <p>
        The Algorithmic Accountability Act is a proposed federal law in the
        United States that is designed to regulate the use of AI systems that
        have a significant impact on individuals&#39; lives. The bill has been
        introduced in Congress in several different forms, but it has not yet
        been passed into law. The bill is designed to address the potential for
        AI systems to be biased or discriminatory, and it would require
        companies to conduct impact assessments of their AI systems to identify
        and mitigate these risks. The bill would also require companies to be
        more transparent about their use of AI and to provide individuals with
        certain rights, such as the right to know when an AI system is being
        used to make a decision about them.
      </p>
      <p>
        The Algorithmic Accountability Act is a significant piece of
        legislation, as it would be the first federal law in the United States
        to specifically regulate the use of AI. The bill has been endorsed by a
        number of civil rights and consumer advocacy groups, who argue that it
        is necessary to protect individuals from the potential harms of AI.
        However, the bill has also been opposed by some industry groups, who
        argue that it would be too burdensome and would stifle innovation. The
        bill is still in the legislative process, and it is not yet clear
        whether it will be passed into law. However, it is a clear indication
        that there is a growing interest in AI regulation at the federal level
        in the United States.
      </p>
      <h3 id="2-4-state-level-ai-regulations-and-proposals">
        2.4. State-Level AI Regulations and Proposals
      </h3>
      <p>
        In addition to the laws that have been passed in California, a number of
        other states are also considering or have passed laws that regulate the
        use of AI. These laws are often focused on specific sectors or use
        cases, such as the use of AI in hiring, housing, or credit. For example,
        Illinois has passed a law that regulates the use of AI in video
        interviews, and New York City has passed a law that requires companies
        to conduct bias audits of their AI-powered hiring tools. These laws are
        a clear indication that there is a growing interest in AI regulation at
        the state level in the United States.
      </p>
      <p>
        The patchwork of state-level AI regulations is creating a complex and
        challenging compliance landscape for tech companies. Companies that
        operate in multiple states must be aware of the different laws that
        apply in each state and must ensure that they are in compliance with all
        of them. This can be a difficult and costly process, and it is likely to
        become even more challenging as more states pass AI-related laws. The
        lack of a uniform federal standard for AI regulation is a major
        challenge for the tech industry, and it is a key reason why many
        companies are calling for a more comprehensive and consistent approach
        to AI regulation at the federal level.
      </p>
      <h2 id="3-global-ai-governance-landscape">
        3. Global AI Governance Landscape
      </h2>
      <p>
        The global landscape of AI governance is rapidly evolving, with
        countries around the world taking different approaches to regulating
        this transformative technology. The European Union&#39;s AI Act has set
        a new global standard for AI regulation, and it is likely to influence
        the development of AI laws in other parts of the world . However, there
        is no single, unified approach to AI governance, and different countries
        are taking different paths. Some countries, such as China, are taking a
        more centralized and state-controlled approach to AI regulation, while
        others, such as the United Kingdom, are taking a more principles-based
        and industry-led approach. The United States, as discussed above, is
        taking a more fragmented and sectoral approach. The lack of a global
        consensus on AI governance is a major challenge for the tech industry,
        as it creates a complex and uncertain regulatory landscape.
      </p>
      <p>
        Despite the lack of a global consensus, there are a number of
        international initiatives that are aimed at promoting a more coordinated
        approach to AI governance. The G7, the G20, and the OECD are all
        actively engaged in discussions on AI governance, and they have all
        issued principles and guidelines for the responsible development and use
        of AI. These initiatives are not legally binding, but they are helping
        to shape the global conversation on AI governance and to promote a more
        consistent and coherent approach to AI regulation. The development of
        international standards for AI is also a key part of the global effort
        to promote responsible AI. Organizations such as the International
        Organization for Standardization (ISO) and the International
        Electrotechnical Commission (IEC) are developing a range of standards
        for AI, which are designed to provide a common framework for the design,
        development, and deployment of AI systems.
      </p>
      <h3 id="3-1-china-s-approach-to-ai-regulation">
        3.1. China&#39;s Approach to AI Regulation
      </h3>
      <p>
        China has taken a proactive and centralized approach to AI regulation,
        with a focus on promoting the development of a &quot;safe, reliable, and
        controllable&quot; AI industry . The Chinese government has issued a
        number of laws and regulations that are designed to govern the use of
        AI, with a particular focus on algorithmic transparency, content
        moderation, and data security. The most significant of these is the{" "}
        <strong>
          Interim Measures for the Management of Generative AI Services
        </strong>
        , which was first introduced in 2023 and which continues to shape AI
        governance in China . This law requires companies that provide
        generative AI services to register their algorithms with the government
        and to ensure that their services are not used to spread harmful or
        illegal content. The law also requires companies to conduct security
        assessments of their AI systems and to take measures to protect the
        personal information of users.
      </p>
      <p>
        In addition to the Interim Measures for the Management of Generative AI
        Services, China has also implemented a number of sector-specific AI
        rules for industries such as finance, healthcare, and automotive . These
        rules are designed to address the specific risks and challenges that are
        associated with the use of AI in these sectors. For example, the Banking
        and Insurance Institutions Data Security Management Measures require
        financial institutions to take effective measures to protect the
        legitimate rights and interests of individuals and to ensure
        transparency, fairness, and impartiality in their use of AI . China is
        also in the process of developing a comprehensive &quot;AI Act,&quot;
        which is expected to adopt a risk-based approach to assessing AI-based
        technologies, similar to the EU AI Act . However, recent revisions to
        Beijing&#39;s legislative agenda suggest a pivot to rules promoting the
        &quot;healthy development of artificial intelligence&quot; rather than
        more formalistic controls on its deployment .
      </p>
      <h3 id="3-2-the-uk-s-principles-based-regulatory-framework">
        3.2. The UK&#39;s Principles-Based Regulatory Framework
      </h3>
      <p>
        The United Kingdom has adopted a principles-based and industry-led
        approach to AI regulation, which is designed to promote innovation while
        also addressing the potential risks and harms of AI. The UK&#39;s
        approach is set out in a white paper, &quot;A pro-innovation approach to
        AI regulation,&quot; which was published in March 2023. The white paper
        proposes a framework that is based on a set of high-level principles,
        rather than on a rigid set of rules. The principles are:{" "}
        <strong>
          safety, security and robustness; appropriate transparency and
          explainability; fairness; accountability and governance; and
          contestability and redress
        </strong>
        . The white paper proposes that these principles should be implemented
        by existing regulators, such as the Financial Conduct Authority (FCA)
        and the Information Commissioner&#39;s Office (ICO), who are best placed
        to understand the specific risks and challenges that are associated with
        the use of AI in their respective sectors.
      </p>
      <p>
        The UK&#39;s principles-based approach is designed to be a flexible and
        adaptable framework that can keep pace with the rapid development of AI.
        It is also designed to be a pro-innovation approach that does not stifle
        the development of new and beneficial AI technologies. The UK government
        has established a number of initiatives to support its principles-based
        approach, including the AI Council, which is an independent expert
        committee that provides advice to the government on AI policy, and the
        Centre for Data Ethics and Innovation (CDEI), which is a government
        expert body that advises on the ethical and responsible use of data and
        AI. The UK is also actively engaged in international discussions on AI
        governance, and it is working with other countries to promote a more
        coordinated and consistent approach to AI regulation.
      </p>
      <h3 id="3-3-the-g20-ai-process-and-international-cooperation">
        3.3. The G20 AI Process and International Cooperation
      </h3>
      <p>
        The G20, a forum of the world&#39;s major economies, has also been
        actively engaged in discussions on AI governance. The G20 has recognized
        the transformative potential of AI and the need for a coordinated
        international response to the challenges and opportunities that it
        presents. The G20 has issued a number of statements and principles on
        AI, including the <strong>G20 AI Principles</strong>, which were adopted
        in 2019. The G20 AI Principles are based on the OECD AI Principles and
        they provide a set of high-level guidelines for the responsible
        development and use of AI. The principles are: inclusive growth,
        sustainable development and well-being; human-centered values and
        fairness; transparency and explainability; robustness, security and
        safety; and accountability.
      </p>
      <p>
        The G20 has also established a number of working groups and initiatives
        to promote international cooperation on AI. These include the G20
        Digital Economy Task Force, which is responsible for overseeing the
        G20&#39;s work on AI, and the G20 AI for Sustainable Development Summit,
        which is a forum for sharing best practices and for promoting the use of
        AI to achieve the UN Sustainable Development Goals. The G20&#39;s work
        on AI is an important part of the global effort to promote a more
        coordinated and consistent approach to AI governance. The G20&#39;s
        principles and initiatives are helping to shape the global conversation
        on AI and to promote a more responsible and ethical approach to the
        development and use of this transformative technology.
      </p>
      <h3 id="3-4-the-role-of-international-standards-organizations">
        3.4. The Role of International Standards Organizations
      </h3>
      <p>
        International standards organizations, such as the International
        Organization for Standardization (ISO) and the International
        Electrotechnical Commission (IEC), are playing an increasingly important
        role in the global governance of AI. These organizations are developing
        a range of standards for AI, which are designed to provide a common
        framework for the design, development, and deployment of AI systems. The
        standards are not legally binding, but they are widely seen as best
        practices for AI governance and are likely to be used as a reference
        point for future AI regulations. The standards are developed by a
        consensus-based process, with input from a wide range of stakeholders,
        including industry, government, and civil society.
      </p>
      <p>
        The ISO and IEC have already published a number of standards for AI, and
        they are currently working on a number of others. The most significant
        of these is <strong>ISO/IEC 42001</strong>, which is the first
        international standard for AI management systems . This standard
        provides a framework for organizations to establish, implement,
        maintain, and continually improve an AI management system. The standard
        is based on a risk-based approach and it addresses a range of issues,
        including ethical responsibilities, transparency, and accountability.
        Other important standards that are being developed by the ISO and IEC
        include ISO/IEC 23894, which provides guidance on risk management for AI
        systems, and ISO/IEC 42005, which provides guidance on how to conduct AI
        system impact assessments . The development of these standards is a
        crucial part of the global effort to promote a more responsible and
        ethical approach to the development and use of AI.
      </p>
      <h2 id="4-a-2025-compliance-checklist-for-tech-companies">
        4. A 2025 Compliance Checklist for Tech Companies
      </h2>
      <p>
        As the global regulatory landscape for AI continues to evolve, it is
        essential for tech companies to have a clear and comprehensive
        compliance strategy. The following checklist provides a set of key
        actions that tech companies should take to ensure that they are prepared
        for the new AI governance requirements that are coming into effect in
        2025 and beyond. The checklist is based on a review of the key AI
        regulations that are being implemented around the world, including the
        EU AI Act, the California Privacy Rights Act, and the NIST AI Risk
        Management Framework. The checklist is designed to be a practical tool
        that can be used by companies of all sizes and in all sectors to assess
        their current level of compliance and to identify areas where they need
        to improve.
      </p>
      <p>
        The checklist is organized into ten key areas: regulatory alignment by
        jurisdiction, bias mitigation and fairness auditing, explainability and
        transparency, secure data collection and consent management, continuous
        model monitoring and lifecycle audits, incident response and redress
        mechanisms, third-party accountability and vendor management, employee
        training and AI literacy, algorithmic accountability and documentation,
        and sustainability and environmental impact. Each of these areas is
        critical for ensuring that a company&#39;s AI systems are developed and
        used in a way that is responsible, ethical, and compliant with the law.
        The checklist provides a set of specific actions that companies can take
        in each of these areas, as well as a set of tips and best practices. The
        checklist is not a substitute for legal advice, but it is a useful
        starting point for any company that is looking to develop a
        comprehensive and effective AI compliance strategy.
      </p>
      <h3 id="4-1-regulatory-alignment-by-jurisdiction">
        4.1. Regulatory Alignment by Jurisdiction
      </h3>
      <p>
        The first and most important step in developing an AI compliance
        strategy is to understand the regulatory landscape in the jurisdictions
        in which a company operates. The global regulatory landscape for AI is
        complex and fragmented, with different countries and regions taking
        different approaches to regulation. It is essential for companies to
        have a clear understanding of the specific laws and regulations that
        apply to them, as well as the key deadlines and compliance requirements.
        This will require a thorough review of the relevant legislation, as well
        as a close monitoring of any new developments in the regulatory
        landscape. Companies should also consider seeking legal advice to ensure
        that they have a complete and accurate understanding of their compliance
        obligations.
      </p>
      <p>
        The key regulations that companies should be aware of include the{" "}
        <strong>EU AI Act</strong>, the{" "}
        <strong>California Privacy Rights Act (CPRA)</strong> , and the{" "}
        <strong>NIST AI Risk Management Framework</strong>. The EU AI Act is a
        comprehensive, risk-based regulation that will have a significant impact
        on any company that develops or deploys AI systems in the EU . The CPRA
        is a state-level law in California that imposes strict requirements on
        companies that collect and use personal information, and it has
        significant implications for AI systems . The NIST AI Risk Management
        Framework is a voluntary framework that provides guidance on how to
        manage the risks associated with AI, and it is widely seen as a best
        practice for AI governance in the U.S. . In addition to these key
        regulations, companies should also be aware of any other relevant laws
        and regulations in the jurisdictions in which they operate, such as the
        Algorithmic Accountability Act in the U.S. and the Interim Measures for
        the Management of Generative AI Services in China .
      </p>
      <h3 id="4-2-bias-mitigation-and-fairness-auditing">
        4.2. Bias Mitigation and Fairness Auditing
      </h3>
      <p>
        One of the most significant risks associated with AI is the potential
        for bias and discrimination. AI systems are trained on data, and if that
        data is biased, then the AI system will also be biased. This can lead to
        discriminatory outcomes in a wide range of areas, such as hiring,
        lending, and criminal justice. It is therefore essential for companies
        to have a robust process in place for mitigating bias and for auditing
        the fairness of their AI systems. This will require a combination of
        technical and organizational measures, as well as a commitment to a
        culture of fairness and equity.
      </p>
      <p>
        The first step in mitigating bias is to ensure that the data that is
        used to train AI systems is as unbiased as possible. This may involve
        techniques such as data augmentation, data balancing, and the use of
        synthetic data. It is also important to have a diverse team of data
        scientists and engineers who can identify and address potential sources
        of bias in the data. The second step is to use techniques that are
        designed to mitigate bias in the AI models themselves. This may include
        techniques such as adversarial debiasing, fairness-aware machine
        learning, and the use of interpretable models. The third step is to
        conduct regular fairness audits of AI systems to ensure that they are
        not producing discriminatory outcomes. These audits should be conducted
        by an independent third party and should be based on a set of clear and
        objective fairness metrics. There are a number of tools and frameworks
        that can be used to support bias mitigation and fairness auditing, such
        as <strong>IBM AI Fairness 360</strong>,{" "}
        <strong>Microsoft Fairlearn</strong>, and the{" "}
        <strong>Google What-If Tool</strong> .
      </p>
      <h3 id="4-3-explainability-and-transparency">
        4.3. Explainability and Transparency
      </h3>
      <p>
        The &quot;black box&quot; nature of many AI models is a major challenge
        for AI governance. It can be difficult to understand how these models
        make their decisions, which can make it difficult to identify and
        correct errors, to ensure fairness, and to provide meaningful
        explanations to individuals who are affected by the decisions. It is
        therefore essential for companies to have a robust process in place for
        ensuring that their AI systems are explainable and transparent. This
        will require a combination of technical and organizational measures, as
        well as a commitment to a culture of openness and accountability.
      </p>
      <p>
        The first step in ensuring explainability is to use techniques that are
        designed to make AI models more interpretable. This may include
        techniques such as{" "}
        <strong>LIME (Local Interpretable Model-agnostic Explanations)</strong>{" "}
        , <strong>SHAP (SHapley Additive exPlanations)</strong> , and the use of
        model cards . These techniques can help to provide insights into how a
        model is making its decisions, which can be useful for debugging, for
        ensuring fairness, and for providing explanations to individuals. The
        second step is to ensure that there is a clear and transparent process
        for documenting the design, development, and deployment of AI systems.
        This documentation should be comprehensive and should be made available
        to regulators and other stakeholders upon request. The third step is to
        provide clear and meaningful information to individuals who are affected
        by the decisions of AI systems. This information should be easy to
        understand and should explain how the decision was made, what data was
        used, and what the individual&#39;s rights are.
      </p>
      <h3 id="4-4-secure-data-collection-and-consent-management">
        4.4. Secure Data Collection and Consent Management
      </h3>
      <p>
        Data is the lifeblood of AI, and the collection and use of data is a
        critical area of AI governance. The increasing number of data privacy
        laws around the world, such as the GDPR in Europe and the CPRA in
        California, are making it more important than ever for companies to have
        a robust process in place for secure data collection and consent
        management. This will require a combination of technical and
        organizational measures, as well as a commitment to a culture of privacy
        and data protection.
      </p>
      <p>
        The first step in secure data collection is to ensure that data is
        collected in a way that is lawful, fair, and transparent. This means
        that companies must have a clear and lawful basis for collecting data,
        and they must provide individuals with clear and meaningful information
        about how their data will be used. The second step is to ensure that
        data is collected with the informed and revocable consent of the
        individual. This means that individuals must be given a clear and easy
        way to opt out of the collection and use of their data. The third step
        is to ensure that data is stored and processed in a way that is secure
        and that protects the privacy of individuals. This may involve
        techniques such as data encryption, access controls, and data
        anonymization. There are a number of tools and frameworks that can be
        used to support secure data collection and consent management, such as{" "}
        <strong>OneTrust</strong> and <strong>Osano</strong> .
      </p>
      <h3 id="4-5-continuous-model-monitoring-and-lifecycle-audits">
        4.5. Continuous Model Monitoring and Lifecycle Audits
      </h3>
      <p>
        AI systems are not static; they are constantly learning and evolving.
        This means that it is not enough to simply test an AI system once and
        then forget about it. It is essential for companies to have a robust
        process in place for continuously monitoring the performance of their AI
        systems and for conducting regular lifecycle audits. This will help to
        ensure that AI systems are performing as expected, that they are not
        producing biased or discriminatory outcomes, and that they are not being
        used for malicious purposes.
      </p>
      <p>
        The first step in continuous model monitoring is to establish a set of
        key performance indicators (KPIs) that can be used to track the
        performance of an AI system. These KPIs should be aligned with the
        business objectives of the system and should be regularly reviewed and
        updated. The second step is to implement a system for real-time
        monitoring of the AI system&#39;s performance. This may involve the use
        of automated tools that can detect anomalies and drift in the
        model&#39;s performance. The third step is to conduct regular lifecycle
        audits of the AI system. These audits should be conducted by an
        independent third party and should be based on a set of clear and
        objective criteria. The audits should cover the entire lifecycle of the
        AI system, from design and development to deployment and monitoring.
        There are a number of tools and frameworks that can be used to support
        continuous model monitoring and lifecycle audits, such as{" "}
        <strong>Arize AI</strong>, <strong>MLflow</strong>, and{" "}
        <strong>Weights &amp; Biases</strong> .
      </p>
      <h3 id="4-6-incident-response-and-redress-mechanisms">
        4.6. Incident Response and Redress Mechanisms
      </h3>
      <p>
        Despite the best efforts of companies to develop and deploy AI systems
        in a safe and responsible manner, there is always the potential for
        something to go wrong. AI systems can make mistakes, they can be biased,
        and they can be used for malicious purposes. It is therefore essential
        for companies to have a robust process in place for responding to
        AI-related incidents and for providing redress to individuals who have
        been harmed by their AI systems. This will require a combination of
        technical and organizational measures, as well as a commitment to a
        culture of accountability and responsibility.
      </p>
      <p>
        The first step in incident response is to have a clear and well-defined
        process for detecting, reporting, and responding to AI-related
        incidents. This process should be documented and should be regularly
        tested and updated. The second step is to have a clear and well-defined
        process for providing redress to individuals who have been harmed by an
        AI system. This may involve a range of remedies, such as compensation,
        an apology, or a commitment to take steps to prevent the incident from
        happening again. The third step is to have a clear and well-defined
        process for learning from AI-related incidents. This may involve
        conducting a root cause analysis of the incident and taking steps to
        improve the company&#39;s AI governance processes. It is also important
        to have a clear line of accountability for AI-related incidents, and to
        ensure that there is a senior executive who is responsible for
        overseeing the company&#39;s incident response and redress processes .
      </p>
      <h3 id="4-7-third-party-accountability-and-vendor-management">
        4.7. Third-Party Accountability and Vendor Management
      </h3>
      <p>
        Many companies rely on third-party vendors to provide them with AI
        systems and services. This can be a cost-effective way to access the
        latest AI technologies, but it also introduces a new set of risks and
        challenges. It is essential for companies to have a robust process in
        place for managing their relationships with third-party vendors and for
        ensuring that they are accountable for the AI systems and services that
        they provide. This will require a combination of technical and
        organizational measures, as well as a commitment to a culture of due
        diligence and risk management.
      </p>
      <p>
        The first step in third-party accountability is to conduct a thorough
        due diligence process before engaging a new vendor. This process should
        include a review of the vendor&#39;s AI governance processes, as well as
        an assessment of the risks associated with their AI systems and
        services. The second step is to have a clear and well-defined contract
        with the vendor that sets out the responsibilities of each party. The
        contract should include provisions on data privacy, security, and
        intellectual property, as well as provisions on liability and
        indemnification. The third step is to have a clear and well-defined
        process for monitoring the performance of the vendor and for ensuring
        that they are complying with their contractual obligations. This may
        involve regular audits and reviews of the vendor&#39;s AI systems and
        services. It is also important to have a clear and well-defined process
        for terminating the relationship with a vendor if they are not meeting
        their obligations .
      </p>
      <h3 id="4-8-employee-training-and-ai-literacy">
        4.8. Employee Training and AI Literacy
      </h3>
      <p>
        AI is a complex and rapidly evolving field, and it is essential for
        companies to have a workforce that is equipped with the knowledge and
        skills to develop and deploy AI systems in a safe and responsible
        manner. This will require a commitment to a culture of continuous
        learning and development, as well as a robust process for employee
        training and AI literacy. The EU AI Act, for example, requires that
        companies ensure that their employees have a sufficient level of AI
        literacy . This is not just a legal requirement; it is also a business
        imperative. A workforce that is well-trained in AI will be better able
        to identify and mitigate the risks of AI, to innovate in a responsible
        manner, and to build trust with customers and other stakeholders.
      </p>
      <p>
        The first step in employee training and AI literacy is to conduct a
        needs assessment to identify the specific knowledge and skills that are
        required for different roles within the company. This will help to
        ensure that the training is targeted and effective. The second step is
        to develop a comprehensive training program that covers all aspects of
        AI, from the technical fundamentals to the ethical and legal
        implications. The training program should be tailored to the specific
        needs of the company and should be regularly updated to keep pace with
        the latest developments in the field. The third step is to create a
        cross-functional AI ethics committee that is responsible for overseeing
        the company&#39;s AI governance processes and for providing guidance and
        support to employees. The committee should be made up of representatives
        from different parts of the company, including legal, compliance,
        engineering, and business. It is also important to create a culture
        where employees feel comfortable raising concerns about the ethical and
        legal implications of AI, and to have a clear and well-defined process
        for addressing these concerns .
      </p>
      <h3 id="4-9-algorithmic-accountability-and-documentation">
        4.9. Algorithmic Accountability and Documentation
      </h3>
      <p>
        Algorithmic accountability is a core principle of AI governance, and it
        is essential for ensuring that AI systems are developed and used in a
        way that is fair, transparent, and accountable. This will require a
        combination of technical and organizational measures, as well as a
        commitment to a culture of documentation and record-keeping. The EU AI
        Act, for example, requires that providers of high-risk AI systems create
        and maintain comprehensive technical documentation and records . This
        documentation must be made available to regulators upon request and must
        be provided to the deployer of the AI system. The documentation must be
        clear, comprehensive, and easy to understand, and it must provide a
        complete and accurate picture of the AI system, from its initial design
        to its final deployment and use.
      </p>
      <p>
        The first step in algorithmic accountability is to have a clear and
        well-defined process for documenting the design, development, and
        deployment of AI systems. This documentation should be comprehensive and
        should be kept up-to-date throughout the lifecycle of the AI system. The
        second step is to have a clear and well-defined process for version
        control and for tracking changes to the AI system. This will help to
        ensure that there is a clear and auditable trail of all changes that
        have been made to the system. The third step is to have a clear and
        well-defined process for providing explanations of the AI system&#39;s
        decisions. This may involve the use of techniques such as LIME and SHAP,
        as well as the use of model cards. It is also important to have a clear
        and well-defined process for responding to requests for information from
        regulators and other stakeholders .
      </p>
      <h3 id="4-10-sustainability-and-environmental-impact">
        4.10. Sustainability and Environmental Impact
      </h3>
      <p>
        The development and use of AI can have a significant environmental
        impact, particularly in terms of energy consumption. The training of
        large AI models can require a vast amount of computational power, which
        can lead to a significant carbon footprint. It is therefore essential
        for companies to have a robust process in place for measuring and
        managing the environmental impact of their AI systems. This will require
        a combination of technical and organizational measures, as well as a
        commitment to a culture of sustainability and environmental
        responsibility.
      </p>
      <p>
        The first step in managing the environmental impact of AI is to measure
        the energy consumption of AI systems. This may involve the use of tools
        that can track the energy consumption of the hardware that is used to
        train and run AI models. The second step is to optimize the energy
        efficiency of AI systems. This may involve the use of more efficient
        algorithms and models, as well as the use of more energy-efficient
        hardware. The third step is to consider the use of green cloud providers
        or on-premise renewable energy options. This can help to reduce the
        carbon footprint of AI systems and to promote a more sustainable
        approach to AI. It is also important to have a clear and well-defined
        process for reporting on the environmental impact of AI systems, and to
        be transparent with customers and other stakeholders about the
        company&#39;s commitment to sustainability .
      </p>
      <h2 id="5-key-sectors-and-use-cases-under-scrutiny">
        5. Key Sectors and Use Cases Under Scrutiny
      </h2>
      <p>
        The new wave of AI governance is not uniform across all industries.
        Regulators are paying particularly close attention to sectors where
        AI-driven decisions can have a profound and immediate impact on
        individuals&#39; lives, rights, and opportunities. These
        &quot;high-stakes&quot; domains are often the first to be addressed by
        new legislation and are subject to the most stringent compliance
        requirements. For tech companies operating in these areas, understanding
        the specific risks and regulatory expectations is paramount.
      </p>
      <h3 id="5-1-financial-services-and-credit-scoring">
        5.1. Financial Services and Credit Scoring
      </h3>
      <p>
        The financial sector is a primary focus of AI regulation due to the high
        potential for algorithmic bias to perpetuate and amplify existing
        socioeconomic inequalities. AI systems used for credit scoring, loan
        approvals, and insurance underwriting are considered{" "}
        <strong>high-risk</strong> under the EU AI Act and are subject to its
        full suite of obligations . This includes requirements for robust data
        governance to ensure training data is representative and free from bias,
        as well as mandates for transparency and explainability so that
        applicants can understand why they were denied credit. In the U.S., the
        use of AI in lending is also under scrutiny from regulators like the
        Consumer Financial Protection Bureau (CFPB), which has warned that
        companies must be able to explain the reasoning behind their credit
        decisions to ensure compliance with fair lending laws. The challenge for
        fintech companies is to build models that are not only accurate but also
        demonstrably fair and non-discriminatory across different demographic
        groups.
      </p>
      <h3 id="5-2-healthcare-and-medical-devices">
        5.2. Healthcare and Medical Devices
      </h3>
      <p>
        AI&#39;s application in healthcare, from diagnostic tools to
        personalized treatment plans, offers immense promise but also carries
        significant risks. AI systems that are used as safety components in
        medical devices or for critical diagnostic purposes are classified as{" "}
        <strong>high-risk</strong> under the EU AI Act . This means they must
        undergo rigorous conformity assessments and meet strict standards for
        accuracy, robustness, and clinical validation. The U.S. Food and Drug
        Administration (FDA) is also actively developing a regulatory framework
        for AI/ML-based medical devices, focusing on ensuring their safety and
        effectiveness throughout their lifecycle. Key concerns include the
        potential for algorithmic bias to lead to misdiagnosis in
        underrepresented populations and the need for clear human oversight to
        prevent over-reliance on automated recommendations. Companies developing
        AI for healthcare must navigate a complex web of medical device
        regulations, data privacy laws (like HIPAA in the U.S.), and emerging
        AI-specific rules.
      </p>
      <h3 id="5-3-employment-and-hr-tech">5.3. Employment and HR Tech</h3>
      <p>
        The use of AI in hiring, performance evaluation, and employee management
        is another area of intense regulatory focus. AI-powered tools that
        screen resumes, conduct video interviews, or assess worker productivity
        are considered <strong>high-risk</strong> because they can directly
        impact individuals&#39; livelihoods and career prospects . The EU AI Act
        imposes strict requirements on these systems to prevent discrimination
        and ensure fairness. In the U.S., the Equal Employment Opportunity
        Commission (EEOC) has issued guidance warning that the use of AI in
        hiring can violate federal anti-discrimination laws if it results in a
        disparate impact on protected groups. New state and local laws, such as
        New York City&#39;s law requiring bias audits for AI hiring tools, are
        adding another layer of compliance complexity. HR tech companies must
        ensure their algorithms are not only effective but also fair,
        transparent, and subject to meaningful human oversight in the
        decision-making process.
      </p>
      <h3 id="5-4-law-enforcement-and-public-services">
        5.4. Law Enforcement and Public Services
      </h3>
      <p>
        The use of AI by law enforcement and government agencies is one of the
        most sensitive areas of AI governance, with significant implications for
        civil liberties and human rights. The EU AI Act takes a particularly
        strong stance, <strong>prohibiting</strong> the use of real-time
        biometric identification systems in public spaces by law enforcement,
        with very limited exceptions . It also classifies AI systems used for
        risk assessment in the criminal justice system or for evaluating the
        reliability of evidence as <strong>high-risk</strong>, subjecting them
        to strict oversight. In the U.S., the use of facial recognition
        technology by law enforcement has been a subject of intense debate and
        has led to bans or moratoriums in several cities and states. The primary
        concerns are the potential for mass surveillance, the chilling effect on
        free expression and assembly, and the risk of misidentification, which
        can have devastating consequences for individuals. Companies providing
        AI tools to the public sector must navigate a highly charged political
        and ethical landscape, with a strong emphasis on accountability,
        transparency, and the protection of fundamental rights.
      </p>
      <h3 id="5-5-generative-ai-and-content-creation">
        5.5. Generative AI and Content Creation
      </h3>
      <p>
        The rapid rise of generative AI models, such as large language models
        (LLMs) and image generators, has created a new and urgent set of
        governance challenges. These models are capable of creating highly
        realistic text, images, and audio, which can be used for both beneficial
        and malicious purposes. The EU AI Act addresses this by creating a
        specific category for <strong>General-Purpose AI (GPAI) models</strong>,
        with tiered obligations based on systemic risk . Providers of these
        models must be transparent about their training data and capabilities,
        and those with systemic risk face additional requirements for evaluation
        and risk mitigation. Key concerns include the potential for generating
        misinformation and deepfakes at scale, copyright infringement in
        training data, and the amplification of biases present in the vast
        datasets used to train these models. For tech companies developing or
        deploying generative AI, the focus is on building in safeguards to
        prevent misuse, ensuring transparency about AI-generated content, and
        respecting intellectual property rights.
      </p>
    </>
  );
}
